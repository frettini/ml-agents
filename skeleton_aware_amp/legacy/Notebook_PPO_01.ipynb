{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('mla-env': venv)"
  },
  "interpreter": {
   "hash": "98f98e15727b138dbe83789526665a788b00fc73f5809a42725d22ab43def4cd"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import mlagents\n",
    "from mlagents_envs.registry import default_registry\n",
    "from mlagents_envs.environment import ActionTuple\n",
    "from mlagents.trainers.settings import TorchSettings\n",
    "\n",
    "from mlagents.torch_utils import torch, default_device, set_torch_config\n",
    "from mlagents.plugins.ppo.PPO import PPO, RolloutBuffer\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "set_torch_config(TorchSettings(device='cpu'))\n",
    "print(default_device())"
   ]
  },
  {
   "source": [
    "### Load a default scene\n",
    "Load a default scene from the registry and reset it"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = '3DBall'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hard_env_reset():\n",
    "    try:\n",
    "        env.close()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return default_registry[env_id].make()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = default_registry[env_id].make()#hard_env_reset()\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "wtf\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "# collect trajectory only for agent 1 \n",
    "done = False\n",
    "behavior_name = list(env.behavior_specs)[0]\n",
    "spec = env.behavior_specs[behavior_name]\n",
    "env.reset()\n",
    "step = 0\n",
    "max_training_timesteps = 200\n",
    "ppo_agent.buffer.clear()\n",
    "while step < max_training_timesteps:\n",
    "    \n",
    "    decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "    \n",
    "    if 0 in terminal_steps.agent_id:\n",
    "        done = True\n",
    "        ppo_agent.buffer.rewards.append(terminal_steps[index].reward)\n",
    "        ppo_agent.buffer.is_terminals.append(done)\n",
    "\n",
    "        obs = terminal_steps[index].obs[0]\n",
    "        action = ppo_agent.select_action(obs)\n",
    "\n",
    "    env.set_actions(behavior_name, spec.action_spec.empty_action(len(decision_steps)))\n",
    "\n",
    "    if 0 in decision_steps.agent_id:\n",
    "        done = False\n",
    "\n",
    "        ppo_agent.buffer.rewards.append(decision_steps[index].reward)\n",
    "        ppo_agent.buffer.is_terminals.append(done)\n",
    "\n",
    "        # get action\n",
    "        obs = decision_steps[index].obs[0]\n",
    "        action = ppo_agent.select_action(obs)\n",
    "        action_tuple = ActionTuple()\n",
    "        action_tuple.add_continuous(action[np.newaxis,:])\n",
    "\n",
    "        env.set_action_for_agent(behavior_name, 0, action_tuple)\n",
    "\n",
    "    if 0 in terminal_steps.agent_id and 0 in decision_steps.agent_id:\n",
    "        print(\"wtf\")\n",
    "\n",
    "    env.step()\n",
    "    step+=1\n",
    "    print(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "source": [
    "state = terminal_steps[index].obs[0]\n",
    "# select action with policy\n",
    "action = ppo_agent.select_action(state)\n",
    "action_tuple = ActionTuple()\n",
    "action_tuple.add_continuous(action)\n",
    "# print(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_continuous_action_space = True  # continuous action space; else discrete\n",
    "\n",
    "max_ep_len = 2000                   # max timesteps in one episode\n",
    "max_training_timesteps = int(3e6)   # break training loop if timeteps > max_training_timesteps\n",
    "\n",
    "print_freq = max_ep_len /10        # print avg reward in the interval (in num timesteps)\n",
    "log_freq = max_ep_len          # log avg reward in the interval (in num timesteps)\n",
    "save_model_freq = int(1e5)          # save model frequency (in num timesteps)\n",
    "\n",
    "action_std = 0.6                    # starting std for action distribution (Multivariate Normal)\n",
    "action_std_decay_rate = 0.05        # linearly decay action_std (action_std = action_std - action_std_decay_rate)\n",
    "min_action_std = 0.1                # minimum action_std (stop decay after action_std <= min_action_std)\n",
    "action_std_decay_freq = int(2.5e5)  # action_std decay frequency (in num timesteps)\n",
    "\n",
    "update_timestep = max_ep_len      # update policy every n timesteps\n",
    "K_epochs = 80               # update policy for K epochs in one PPO update\n",
    "\n",
    "eps_clip = 0.2          # clip parameter for PPO\n",
    "gamma = 0.99            # discount factor\n",
    "\n",
    "lr_actor = 0.0003       # learning rate for actor network\n",
    "lr_critic = 0.001       # learning rate for critic network\n",
    "\n",
    "random_seed = 0         # set random seed if required (0 = no random seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, options):\n",
    "\n",
    "    env.reset()\n",
    "    \n",
    "    # get the first behavior name and its spec\n",
    "    behavior_name = list(env.behavior_specs)[0]\n",
    "    spec = env.behavior_specs[behavior_name]\n",
    "\n",
    "    # state space dimension\n",
    "    state_dim = spec.observation_specs[0].shape[0]\n",
    "\n",
    "    # action space dimension\n",
    "    action_dim = len(spec.action_spec)\n",
    "\n",
    "    # setup logger\n",
    "    t = time.localtime()\n",
    "    log_dir = './runs/3DBall/first_logs_{}/'.format(time.strftime(\"%d_%M_%Y-%H_%M\", t))\n",
    "    writer = SummaryWriter(log_dir)\n",
    "\n",
    "\n",
    "    run_num_pretrained = 0      #### change this to prevent overwriting weights in same env_name folder\n",
    "\n",
    "    hyperparam_log = \"\"\"\n",
    "    max training timesteps : {}, max timesteps per episode : {}, model saving frequency : {}, \n",
    "    state space dimension : {}, action space dimension : {}, action_std : {}, action decay rate {}, \n",
    "    min action std : {}, decay frequency : {}, PPO update freq : {}, PPO K epochs : {},\n",
    "     Epsilon clip : {}, gamma : {}, Lr actor : {}, Lr Critic : {}\n",
    "    \"\"\".format(max_training_timesteps, max_ep_len, save_model_freq, state_dim, action_dim,  action_std, action_std_decay_rate, min_action_std, action_std_decay_freq, update_timestep, K_epochs, eps_clip, gamma, lr_actor, lr_critic)\n",
    "    \n",
    "    writer.add_text(\"Hyperparameter\", hyperparam_log)\n",
    "\n",
    "    # initialize a PPO agent\n",
    "    ppo_agent = PPO(state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, has_continuous_action_space, action_std)\n",
    "\n",
    "\n",
    "    # track total training time\n",
    "    start_time = datetime.now().replace(microsecond=0)\n",
    "\n",
    "    # printing and logging variables\n",
    "    num_episode = 1\n",
    "    logstep = 0\n",
    "    running_reward =0 \n",
    "\n",
    "    time_step = 0\n",
    "    i_episode = 0\n",
    "    \n",
    "    # training loop\n",
    "    while time_step <= max_training_timesteps:\n",
    "\n",
    "        state = env.reset()\n",
    "        current_ep_reward = 0\n",
    "        print(\"time step:\", time_step)\n",
    "\n",
    "        #TODO: initialize with empty state\n",
    "        for t in range(1, max_ep_len+1):\n",
    "            \n",
    "            decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "\n",
    "            # check if tracked agent has ended episode within step\n",
    "            if 0 in terminal_steps.agent_id:\n",
    "                done = True\n",
    "\n",
    "                index = terminal_steps.agent_id_to_index[0]\n",
    "                reward = terminal_steps[index].reward\n",
    "                ppo_agent.buffer.rewards.append(reward)\n",
    "                ppo_agent.buffer.is_terminals.append(done)\n",
    "\n",
    "                obs = terminal_steps[index].obs[0]\n",
    "                action = ppo_agent.select_action(obs)\n",
    "\n",
    "                num_episode += 1\n",
    "\n",
    "            env.set_actions(behavior_name, spec.action_spec.empty_action(len(decision_steps)))\n",
    "\n",
    "            # check if tracked agent requires a new decision within step\n",
    "            if 0 in decision_steps.agent_id:\n",
    "                done = False\n",
    "\n",
    "                index = decision_steps.agent_id_to_index[0]\n",
    "                reward = decision_steps[index].reward\n",
    "                ppo_agent.buffer.rewards.append(reward)\n",
    "                ppo_agent.buffer.is_terminals.append(done)\n",
    "\n",
    "                # get action\n",
    "                obs = decision_steps[index].obs[0]\n",
    "                action = ppo_agent.select_action(obs)\n",
    "                action_tuple = ActionTuple()\n",
    "                action_tuple.add_continuous(action[np.newaxis,:])\n",
    "\n",
    "                env.set_action_for_agent(behavior_name, 0, action_tuple)\n",
    "\n",
    "\n",
    "            env.step()\n",
    "\n",
    "            time_step +=1\n",
    "            current_ep_reward += reward\n",
    "\n",
    "            # update PPO agent\n",
    "            if time_step % update_timestep == 0:\n",
    "                print(\"Update PPO\")\n",
    "                ppo_agent.update()\n",
    "\n",
    "            # if continuous action space; then decay action std of ouput action distribution\n",
    "            if has_continuous_action_space and time_step % action_std_decay_freq == 0:\n",
    "                ppo_agent.decay_action_std(action_std_decay_rate, min_action_std)\n",
    "\n",
    "            # log in logging file\n",
    "            if time_step % print_freq == 0:\n",
    "                writer.add_scalar(\"Reward per timestep\", current_ep_reward/t ,logstep)\n",
    "                logstep += 1\n",
    "                \n",
    "\n",
    "            # printing average reward\n",
    "            if time_step % print_freq == 0:\n",
    "\n",
    " \n",
    "                print(\"Episode : {} \\t Timestep : {} \\t Current Ep Reward : {} \\t Average Reward : {}\".format(i_episode, time_step, current_ep_reward, current_ep_reward/num_episode))\n",
    "\n",
    "                \n",
    "        writer.add_scalar(\"Reward per Episode\", current_ep_reward ,i_episode)\n",
    "        print(\"End of Episode : {} \\t episode reward: {}\".format(i_episode, current_ep_reward) )\n",
    "        running_reward += current_ep_reward\n",
    "        \n",
    "        num_episode = 0\n",
    "        i_episode += 1\n",
    "\n",
    "    # env.close()\n",
    "\n",
    "\n",
    "    # print total training time\n",
    "    end_time = datetime.now().replace(microsecond=0)\n",
    "    print(\"Started training at (GMT) : \", start_time)\n",
    "    print(\"Finished training at (GMT) : \", end_time)\n",
    "    print(\"Total training time  : \", end_time - start_time)\n",
    "  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cpu\n",
      "cpu\n",
      "cpu\n",
      "time step: 0\n",
      "Episode : 0 \t Timestep : 10 \t Current Ep Reward : 1.0000000149011612 \t Average Reward : 1.0000000149011612\n",
      "Episode : 0 \t Timestep : 20 \t Current Ep Reward : 0.9000000283122063 \t Average Reward : 0.45000001415610313\n",
      "Episode : 0 \t Timestep : 30 \t Current Ep Reward : 1.9000000432133675 \t Average Reward : 0.9500000216066837\n",
      "Episode : 0 \t Timestep : 40 \t Current Ep Reward : 1.8000000566244125 \t Average Reward : 0.6000000188748041\n",
      "Episode : 0 \t Timestep : 50 \t Current Ep Reward : 2.8000000715255737 \t Average Reward : 0.9333333571751913\n",
      "Episode : 0 \t Timestep : 60 \t Current Ep Reward : 3.800000086426735 \t Average Reward : 1.2666666954755783\n",
      "Episode : 0 \t Timestep : 70 \t Current Ep Reward : 4.800000101327896 \t Average Reward : 1.6000000337759654\n",
      "Episode : 0 \t Timestep : 80 \t Current Ep Reward : 4.700000114738941 \t Average Reward : 1.1750000286847353\n",
      "Episode : 0 \t Timestep : 90 \t Current Ep Reward : 5.700000129640102 \t Average Reward : 1.4250000324100256\n",
      "Update PPO\n",
      "Episode : 0 \t Timestep : 100 \t Current Ep Reward : 6.500000141561031 \t Average Reward : 1.3000000283122062\n",
      "End of Episode : 0 \t episode reward: 6.500000141561031\n",
      "time step: 100\n",
      "Episode : 1 \t Timestep : 110 \t Current Ep Reward : 1.0000000149011612 \t Average Reward : 0.20000000298023224\n",
      "Episode : 1 \t Timestep : 120 \t Current Ep Reward : 0.9000000283122063 \t Average Reward : 0.15000000471870104\n",
      "Episode : 1 \t Timestep : 130 \t Current Ep Reward : 1.9000000432133675 \t Average Reward : 0.3166666738688946\n",
      "Episode : 1 \t Timestep : 140 \t Current Ep Reward : 2.9000000581145287 \t Average Reward : 0.4833333430190881\n",
      "Episode : 1 \t Timestep : 150 \t Current Ep Reward : 2.8000000715255737 \t Average Reward : 0.40000001021793913\n",
      "Episode : 1 \t Timestep : 160 \t Current Ep Reward : 3.800000086426735 \t Average Reward : 0.5428571552038193\n",
      "Episode : 1 \t Timestep : 170 \t Current Ep Reward : 4.800000101327896 \t Average Reward : 0.6857143001896995\n",
      "Episode : 1 \t Timestep : 180 \t Current Ep Reward : 5.700000114738941 \t Average Reward : 0.7125000143423676\n",
      "Episode : 1 \t Timestep : 190 \t Current Ep Reward : 6.700000129640102 \t Average Reward : 0.8375000162050128\n",
      "Update PPO\n",
      "Episode : 1 \t Timestep : 200 \t Current Ep Reward : 7.700000144541264 \t Average Reward : 0.962500018067658\n",
      "End of Episode : 1 \t episode reward: 7.700000144541264\n",
      "time step: 200\n",
      "Episode : 2 \t Timestep : 210 \t Current Ep Reward : 1.0000000149011612 \t Average Reward : 0.12500000186264515\n",
      "Episode : 2 \t Timestep : 220 \t Current Ep Reward : 0.9000000283122063 \t Average Reward : 0.1000000031458007\n",
      "Episode : 2 \t Timestep : 230 \t Current Ep Reward : 0.8000000417232513 \t Average Reward : 0.08000000417232514\n",
      "Episode : 2 \t Timestep : 240 \t Current Ep Reward : 1.8000000566244125 \t Average Reward : 0.18000000566244126\n",
      "Episode : 2 \t Timestep : 250 \t Current Ep Reward : 1.7000000700354576 \t Average Reward : 0.15454546091231433\n",
      "Episode : 2 \t Timestep : 260 \t Current Ep Reward : 2.700000084936619 \t Average Reward : 0.24545455317605624\n",
      "Episode : 2 \t Timestep : 270 \t Current Ep Reward : 2.600000098347664 \t Average Reward : 0.21666667486230531\n",
      "Episode : 2 \t Timestep : 280 \t Current Ep Reward : 3.600000113248825 \t Average Reward : 0.30000000943740207\n",
      "Episode : 2 \t Timestep : 290 \t Current Ep Reward : 4.600000128149986 \t Average Reward : 0.38333334401249886\n",
      "Update PPO\n",
      "Episode : 2 \t Timestep : 300 \t Current Ep Reward : 4.500000141561031 \t Average Reward : 0.34615385704315627\n",
      "End of Episode : 2 \t episode reward: 4.500000141561031\n",
      "time step: 300\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-8ea97c77e8fd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-44-74de2fd30754>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(env, options)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m             \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m             \u001b[0mtime_step\u001b[0m \u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nicol\\work\\master\\dissertation\\ml-agents\\ml-agents-envs\\mlagents_envs\\timers.py\u001b[0m in \u001b[0;36mwrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    303\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mhierarchical_timer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__qualname__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 305\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m  \u001b[1;31m# type: ignore\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nicol\\work\\master\\dissertation\\ml-agents\\ml-agents-envs\\mlagents_envs\\environment.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    331\u001b[0m         \u001b[0mstep_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_generate_step_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_env_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mhierarchical_timer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"communicator.exchange\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 333\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_communicator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexchange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_poll_process\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    334\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mUnityCommunicatorStoppedException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Communicator has exited.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nicol\\work\\master\\dissertation\\ml-agents\\ml-agents-envs\\mlagents_envs\\rpc_communicator.py\u001b[0m in \u001b[0;36mexchange\u001b[1;34m(self, inputs, poll_callback)\u001b[0m\n\u001b[0;32m    135\u001b[0m         \u001b[0mmessage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munity_input\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munity_to_external\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparent_conn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpoll_for_timeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpoll_callback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munity_to_external\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparent_conn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nicol\\work\\master\\dissertation\\ml-agents\\ml-agents-envs\\mlagents_envs\\rpc_communicator.py\u001b[0m in \u001b[0;36mpoll_for_timeout\u001b[1;34m(self, poll_callback)\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[0mcallback_timeout_wait\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout_wait\u001b[0m \u001b[1;33m//\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mdeadline\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munity_to_external\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparent_conn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcallback_timeout_wait\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m                 \u001b[1;31m# Got an acknowledgment from the connection\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m                 \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\multiprocessing\\connection.py\u001b[0m in \u001b[0;36mpoll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 257\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\multiprocessing\\connection.py\u001b[0m in \u001b[0;36m_poll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    328\u001b[0m                         _winapi.PeekNamedPipe(self._handle)[0] != 0):\n\u001b[0;32m    329\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 330\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    331\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_get_more_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mov\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\multiprocessing\\connection.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(object_list, timeout)\u001b[0m\n\u001b[0;32m    877\u001b[0m                         \u001b[0mtimeout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    878\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 879\u001b[1;33m             \u001b[0mready_handles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_exhaustive_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwaithandle_to_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    880\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    881\u001b[0m             \u001b[1;31m# request that overlapped reads stop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\multiprocessing\\connection.py\u001b[0m in \u001b[0;36m_exhaustive_wait\u001b[1;34m(handles, timeout)\u001b[0m\n\u001b[0;32m    809\u001b[0m         \u001b[0mready\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mL\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_winapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWaitForMultipleObjects\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mWAIT_TIMEOUT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(env, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = hard_env_reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}