{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('mla-env': venv)"
  },
  "interpreter": {
   "hash": "98f98e15727b138dbe83789526665a788b00fc73f5809a42725d22ab43def4cd"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import mlagents\n",
    "from mlagents_envs.registry import default_registry\n",
    "from mlagents_envs.environment import ActionTuple\n",
    "from mlagents.trainers.settings import TorchSettings\n",
    "\n",
    "from mlagents.torch_utils import torch, default_device, set_torch_config\n",
    "from mlagents.plugins.ppo.PPO import PPO, RolloutBuffer\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "set_torch_config(TorchSettings(device='cpu'))\n",
    "print(default_device())"
   ]
  },
  {
   "source": [
    "### Load a default scene\n",
    "Load a default scene from the registry and reset it"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = '3DBall'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hard_env_reset():\n",
    "    try:\n",
    "        env.close()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return default_registry[env_id].make()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = hard_env_reset()\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "wtf\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "# collect trajectory only for agent 1 \n",
    "done = False\n",
    "behavior_name = list(env.behavior_specs)[0]\n",
    "spec = env.behavior_specs[behavior_name]\n",
    "env.reset()\n",
    "step = 0\n",
    "max_training_timesteps = 200\n",
    "ppo_agent.buffer.clear()\n",
    "while step < max_training_timesteps:\n",
    "    \n",
    "    decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "    \n",
    "    if 0 in terminal_steps.agent_id:\n",
    "        done = True\n",
    "        ppo_agent.buffer.rewards.append(terminal_steps[index].reward)\n",
    "        ppo_agent.buffer.is_terminals.append(done)\n",
    "\n",
    "        obs = terminal_steps[index].obs[0]\n",
    "        action = ppo_agent.select_action(obs)\n",
    "\n",
    "    env.set_actions(behavior_name, spec.action_spec.empty_action(len(decision_steps)))\n",
    "\n",
    "    if 0 in decision_steps.agent_id:\n",
    "        done = False\n",
    "\n",
    "        ppo_agent.buffer.rewards.append(decision_steps[index].reward)\n",
    "        ppo_agent.buffer.is_terminals.append(done)\n",
    "\n",
    "        # get action\n",
    "        obs = decision_steps[index].obs[0]\n",
    "        action = ppo_agent.select_action(obs)\n",
    "        action_tuple = ActionTuple()\n",
    "        action_tuple.add_continuous(action[np.newaxis,:])\n",
    "\n",
    "        env.set_action_for_agent(behavior_name, 0, action_tuple)\n",
    "\n",
    "    if 0 in terminal_steps.agent_id and 0 in decision_steps.agent_id:\n",
    "        print(\"wtf\")\n",
    "\n",
    "    env.step()\n",
    "    step+=1\n",
    "    print(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "source": [
    "state = terminal_steps[index].obs[0]\n",
    "# select action with policy\n",
    "action = ppo_agent.select_action(state)\n",
    "action_tuple = ActionTuple()\n",
    "action_tuple.add_continuous(action)\n",
    "# print(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_continuous_action_space = True  # continuous action space; else discrete\n",
    "\n",
    "max_ep_len = 100                   # max timesteps in one episode\n",
    "max_training_timesteps = int(3e6)   # break training loop if timeteps > max_training_timesteps\n",
    "\n",
    "print_freq = max_ep_len /10        # print avg reward in the interval (in num timesteps)\n",
    "log_freq = max_ep_len          # log avg reward in the interval (in num timesteps)\n",
    "save_model_freq = int(1e5)          # save model frequency (in num timesteps)\n",
    "\n",
    "action_std = 0.6                    # starting std for action distribution (Multivariate Normal)\n",
    "action_std_decay_rate = 0.05        # linearly decay action_std (action_std = action_std - action_std_decay_rate)\n",
    "min_action_std = 0.1                # minimum action_std (stop decay after action_std <= min_action_std)\n",
    "action_std_decay_freq = int(2.5e5)  # action_std decay frequency (in num timesteps)\n",
    "\n",
    "update_timestep = max_ep_len      # update policy every n timesteps\n",
    "K_epochs = 80               # update policy for K epochs in one PPO update\n",
    "\n",
    "eps_clip = 0.2          # clip parameter for PPO\n",
    "gamma = 0.99            # discount factor\n",
    "\n",
    "lr_actor = 0.0003       # learning rate for actor network\n",
    "lr_critic = 0.001       # learning rate for critic network\n",
    "\n",
    "random_seed = 0         # set random seed if required (0 = no random seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, options):\n",
    "\n",
    "    env.reset()\n",
    "    \n",
    "    # get the first behavior name and its spec\n",
    "    behavior_name = list(env.behavior_specs)[0]\n",
    "    spec = env.behavior_specs[behavior_name]\n",
    "\n",
    "    # state space dimension\n",
    "    state_dim = spec.observation_specs[0].shape[0]\n",
    "\n",
    "    # action space dimension\n",
    "    action_dim = len(spec.action_spec)\n",
    "\n",
    "    # setup logger\n",
    "    t = time.localtime()\n",
    "    log_dir = './runs/beans/first_logs_{}/'.format(time.strftime(\"%d_%M_%Y-%H_%M\", t))\n",
    "    writer = SummaryWriter(log_dir)\n",
    "\n",
    "\n",
    "    run_num_pretrained = 0      #### change this to prevent overwriting weights in same env_name folder\n",
    "\n",
    "    hyperparam_log = \"\"\"\n",
    "    max training timesteps : {}, max timesteps per episode : {}, model saving frequency : {}, state space dimension : {}, action space dimension : {}, action_std : {}, action decay rate {}, min action std : {}, decay frequency : {}, PPO update freq : {}, PPO K epochs : {}, Epsilon clip : {}, gamma : {}, Lr actor : {}, Lr Critic : {}\n",
    "    \"\"\".format(max_training_timesteps, max_ep_len, save_model_freq, state_dim, action_dim,  action_std, action_std_decay_rate, min_action_std, action_std_decay_freq, update_timestep, K_epochs, eps_clip, gamma, lr_actor, lr_critic)\n",
    "\n",
    "\n",
    "    # initialize a PPO agent\n",
    "    ppo_agent = PPO(state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, has_continuous_action_space, action_std)\n",
    "\n",
    "\n",
    "    # track total training time\n",
    "    start_time = datetime.now().replace(microsecond=0)\n",
    "\n",
    "    # printing and logging variables\n",
    "    print_running_reward = 0\n",
    "    print_running_episodes = 1\n",
    "\n",
    "    log_running_reward = 0\n",
    "    log_running_episodes = 1\n",
    "\n",
    "    time_step = 0\n",
    "    i_episode = 0\n",
    "    \n",
    "    # training loop\n",
    "    while time_step <= max_training_timesteps:\n",
    "\n",
    "        state = env.reset()\n",
    "        current_ep_reward = 0\n",
    "        print(\"time step:\", time_step)\n",
    "\n",
    "        #TODO: initialize with empty state\n",
    "        for t in range(1, max_ep_len+1):\n",
    "            \n",
    "            decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "    \n",
    "            if 0 in terminal_steps.agent_id:\n",
    "                done = True\n",
    "\n",
    "                index = terminal_steps.agent_id_to_index[0]\n",
    "                reward = terminal_steps[index].reward\n",
    "                ppo_agent.buffer.rewards.append(reward)\n",
    "                ppo_agent.buffer.is_terminals.append(done)\n",
    "\n",
    "                obs = terminal_steps[index].obs[0]\n",
    "                action = ppo_agent.select_action(obs)\n",
    "\n",
    "            env.set_actions(behavior_name, spec.action_spec.empty_action(len(decision_steps)))\n",
    "\n",
    "            if 0 in decision_steps.agent_id:\n",
    "                done = False\n",
    "\n",
    "                index = decision_steps.agent_id_to_index[0]\n",
    "                reward = decision_steps[index].reward\n",
    "                ppo_agent.buffer.rewards.append(reward)\n",
    "                ppo_agent.buffer.is_terminals.append(done)\n",
    "\n",
    "                # get action\n",
    "                obs = decision_steps[index].obs[0]\n",
    "                action = ppo_agent.select_action(obs)\n",
    "                action_tuple = ActionTuple()\n",
    "                action_tuple.add_continuous(action[np.newaxis,:])\n",
    "\n",
    "                env.set_action_for_agent(behavior_name, 0, action_tuple)\n",
    "\n",
    "\n",
    "            env.step()\n",
    "\n",
    "            time_step +=1\n",
    "            current_ep_reward += reward\n",
    "\n",
    "            # update PPO agent\n",
    "            if time_step % update_timestep == 0:\n",
    "                print(\"Update PPO\")\n",
    "                ppo_agent.update()\n",
    "\n",
    "            # if continuous action space; then decay action std of ouput action distribution\n",
    "            if has_continuous_action_space and time_step % action_std_decay_freq == 0:\n",
    "                ppo_agent.decay_action_std(action_std_decay_rate, min_action_std)\n",
    "\n",
    "            # log in logging file\n",
    "            if time_step % log_freq == 0:\n",
    "\n",
    "                # log average reward till last episode\n",
    "                log_avg_reward = log_running_reward / log_running_episodes\n",
    "                log_avg_reward = round(log_avg_reward, 4)\n",
    "\n",
    "                log_running_reward = 0\n",
    "                log_running_episodes = 1\n",
    "\n",
    "            # printing average reward\n",
    "            if time_step % print_freq == 0:\n",
    "\n",
    "                # print average reward till last episode\n",
    "                print_avg_reward = print_running_reward / print_running_episodes\n",
    "                print_avg_reward = round(print_avg_reward, 2)\n",
    "\n",
    "                print(\"Episode : {} \\t Timestep : {} \\t Current Ep Reward : {} \\t Average Reward : {}\".format(i_episode, time_step, current_ep_reward, print_avg_reward))\n",
    "\n",
    "                print_running_reward = 0\n",
    "                print_running_episodes = 1\n",
    "\n",
    "\n",
    "        print_running_reward += current_ep_reward\n",
    "        print_running_episodes += 1\n",
    "\n",
    "        log_running_reward += current_ep_reward\n",
    "        log_running_episodes += 1\n",
    "\n",
    "        i_episode += 1\n",
    "\n",
    "    # env.close()\n",
    "\n",
    "\n",
    "    # print total training time\n",
    "    end_time = datetime.now().replace(microsecond=0)\n",
    "    print(\"Started training at (GMT) : \", start_time)\n",
    "    print(\"Finished training at (GMT) : \", end_time)\n",
    "    print(\"Total training time  : \", end_time - start_time)\n",
    "  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cpu\n",
      "cpu\n",
      "cpu\n",
      "time step: 0\n",
      "Episode : 0 \t Timestep : 100 \t Current Ep Reward : 6.500000141561031 \t Average Reward : 0.0\n",
      "Episode : 0 \t Timestep : 200 \t Current Ep Reward : 11.700000278651714 \t Average Reward : 0.0\n",
      "Episode : 0 \t Timestep : 300 \t Current Ep Reward : 19.10000041872263 \t Average Reward : 0.0\n",
      "Episode : 0 \t Timestep : 400 \t Current Ep Reward : 23.60000056028366 \t Average Reward : 0.0\n",
      "Episode : 0 \t Timestep : 500 \t Current Ep Reward : 29.20000070333481 \t Average Reward : 0.0\n",
      "Episode : 0 \t Timestep : 600 \t Current Ep Reward : 34.70000084489584 \t Average Reward : 0.0\n",
      "Episode : 0 \t Timestep : 700 \t Current Ep Reward : 42.30000098794699 \t Average Reward : 0.0\n",
      "Episode : 0 \t Timestep : 800 \t Current Ep Reward : 46.80000112950802 \t Average Reward : 0.0\n",
      "Episode : 0 \t Timestep : 900 \t Current Ep Reward : 55.400001272559166 \t Average Reward : 0.0\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-8ea97c77e8fd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-28-cf6418836712>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(env, options)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m                 \u001b[1;31m# log average reward till last episode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m                 \u001b[0mlog_avg_reward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlog_running_reward\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mlog_running_episodes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m                 \u001b[0mlog_avg_reward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_avg_reward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "train(env, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = hard_env_reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}